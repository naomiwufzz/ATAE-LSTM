{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coding:utf-8\n",
    "import os\n",
    "import numpy as np\n",
    "import torch as t\n",
    "from nltk.tokenize import word_tokenize\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('D:\\\\Jupyter\\\\Python\\\\ATAE-LSTM')\n",
    "import Ipynb_importer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Emb(object):\n",
    "    def __init__(self):\n",
    "        # create and init the items below\n",
    "        # self.embedding   string word ==> np.ndarray vector\n",
    "        self.embedding = {}\n",
    "        \n",
    "        # load the pre-trained data\n",
    "        self.root = opt.base_root + opt.embedding_root\n",
    "        f = open(self.root, 'r', encoding='UTF-8')\n",
    "        \n",
    "        l = f.readline()\n",
    "        have_opened = 1\n",
    "        while(l != '' and have_opened<=opt.embedding_load):\n",
    "            # l : \"a 0.1 0.2 0.3 ...\"\n",
    "            if l[-1] == '\\n':\n",
    "                l = l[:-1]\n",
    "            l = l.split(' ')\n",
    "            if not len(l)==opt.hidden_size + 1:\n",
    "                l = f.readline()\n",
    "                continue\n",
    "            \n",
    "            # l[0]  : string word\n",
    "            # l[1:] : list<string> vector\n",
    "            self.embedding[l[0].lower()] = np.array(l[1:], dtype=float)\n",
    "            \n",
    "            if(len(self.embedding)==have_opened):\n",
    "                print('Embedding : have input words : '+str(have_opened))\n",
    "                have_opened *= 2\n",
    "            l = f.readline()\n",
    "            \n",
    "        print('Embedding : have input words : '+str(have_opened))\n",
    "        f.close()\n",
    "        \n",
    "        # create the items to modify and use dynamically below\n",
    "        # self.dictionary    string word ==> int index\n",
    "        # self.words         int index ==> string word\n",
    "        # self.no_pretrained string word ==> int appearance\n",
    "        self.dictionary = {}\n",
    "        self.words = []\n",
    "        self.no_pretrained = {}\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def _get_dic_(self):\n",
    "        return self.dictionary\n",
    "    \n",
    "    def _get_words_(self):\n",
    "        return self.words\n",
    "    \n",
    "    def _make_layer_(self):\n",
    "        weight = []\n",
    "        for word in self.words:\n",
    "            weight.append(self.embedding[word])\n",
    "        weight.append(np.random.uniform(-opt.epsilon, opt.epsilon, opt.hidden_size))\n",
    "        \n",
    "        layer = nn.Embedding.from_pretrained(t.FloatTensor(weight), freeze=False)\n",
    "        \n",
    "        return layer\n",
    "    \n",
    "    def _add_word_(self, sentence):\n",
    "        # para sentence : a string to be tokenized by nltk.tokenize.word_tokenize\n",
    "        sentence = word_tokenize(sentence)\n",
    "        for word in sentence:\n",
    "            word = word.lower()\n",
    "            if word in self.dictionary:\n",
    "                continue\n",
    "            if word in self.embedding:\n",
    "                # add this word into self.dictionary and self.words\n",
    "                self.dictionary[word] = len(self.words)\n",
    "                self.words.append(word)\n",
    "                assert len(self.dictionary) == len(self.words)\n",
    "            else:\n",
    "                # if this no-pretrained word arise for at least opt.word_independence times\n",
    "                # set an indepent embedding for it\n",
    "                if word not in self.no_pretrained:\n",
    "                    self.no_pretrained[word] = 1\n",
    "                else:\n",
    "                    self.no_pretrained[word] += 1\n",
    "                    if self.no_pretrained[word] >= opt.word_independence:\n",
    "                        self.no_pretrained.pop(word)\n",
    "                        self.dictionary[word] = len(self.words)\n",
    "                        self.words.append(word)\n",
    "                        assert len(self.dictionary) == len(self.words)\n",
    "                        \n",
    "                        # set an indepent embedding for it\n",
    "                        # init from U(-ε,ε) \n",
    "                        self.embedding[word] = np.random.uniform(-opt.epsilon, opt.epsilon, opt.hidden_size)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding : have input words : 1\n",
      "Embedding : have input words : 2\n",
      "Embedding : have input words : 4\n",
      "Embedding : have input words : 8\n",
      "Embedding : have input words : 16\n",
      "Embedding : have input words : 32\n",
      "Embedding : have input words : 64\n",
      "Embedding : have input words : 128\n",
      "Embedding : have input words : 256\n",
      "Embedding : have input words : 512\n",
      "Embedding : have input words : 1024\n",
      "Embedding : have input words : 2048\n",
      "Embedding : have input words : 4096\n",
      "Embedding : have input words : 8192\n",
      "Embedding : have input words : 16384\n"
     ]
    }
   ],
   "source": [
    "emb = Emb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'all': 0, 'the': 1, 'and': 2, 'salads': 3, 'were': 4, 'fabulous': 5, ',': 6, 'steak': 7, 'was': 8, 'mouth': 9, 'watering': 10, 'pasta': 11, 'delicious': 12, '!': 13}\n",
      "{'all': 0, 'the': 1, 'and': 2, 'salads': 3, 'were': 4, 'fabulous': 5, ',': 6, 'steak': 7, 'was': 8, 'mouth': 9, 'watering': 10, 'pasta': 11, 'delicious': 12, '!': 13}\n",
      "{'all': 0, 'the': 1, 'and': 2, 'salads': 3, 'were': 4, 'fabulous': 5, ',': 6, 'steak': 7, 'was': 8, 'mouth': 9, 'watering': 10, 'pasta': 11, 'delicious': 12, '!': 13}\n",
      "{'all': 0, 'the': 1, 'and': 2, 'salads': 3, 'were': 4, 'fabulous': 5, ',': 6, 'steak': 7, 'was': 8, 'mouth': 9, 'watering': 10, 'pasta': 11, 'delicious': 12, '!': 13}\n",
      "{'all': 0, 'the': 1, 'and': 2, 'salads': 3, 'were': 4, 'fabulous': 5, ',': 6, 'steak': 7, 'was': 8, 'mouth': 9, 'watering': 10, 'pasta': 11, 'delicious': 12, '!': 13, 'appetizers': 14}\n",
      "{'all': 0, 'the': 1, 'and': 2, 'salads': 3, 'were': 4, 'fabulous': 5, ',': 6, 'steak': 7, 'was': 8, 'mouth': 9, 'watering': 10, 'pasta': 11, 'delicious': 12, '!': 13, 'appetizers': 14}\n",
      "{'all': 0, 'the': 1, 'and': 2, 'salads': 3, 'were': 4, 'fabulous': 5, ',': 6, 'steak': 7, 'was': 8, 'mouth': 9, 'watering': 10, 'pasta': 11, 'delicious': 12, '!': 13, 'appetizers': 14}\n",
      "{'all': 0, 'the': 1, 'and': 2, 'salads': 3, 'were': 4, 'fabulous': 5, ',': 6, 'steak': 7, 'was': 8, 'mouth': 9, 'watering': 10, 'pasta': 11, 'delicious': 12, '!': 13, 'appetizers': 14}\n",
      "{'all': 0, 'the': 1, 'and': 2, 'salads': 3, 'were': 4, 'fabulous': 5, ',': 6, 'steak': 7, 'was': 8, 'mouth': 9, 'watering': 10, 'pasta': 11, 'delicious': 12, '!': 13, 'appetizers': 14}\n",
      "{'all': 0, 'the': 1, 'and': 2, 'salads': 3, 'were': 4, 'fabulous': 5, ',': 6, 'steak': 7, 'was': 8, 'mouth': 9, 'watering': 10, 'pasta': 11, 'delicious': 12, '!': 13, 'appetizers': 14}\n",
      "{'all': 0, 'the': 1, 'and': 2, 'salads': 3, 'were': 4, 'fabulous': 5, ',': 6, 'steak': 7, 'was': 8, 'mouth': 9, 'watering': 10, 'pasta': 11, 'delicious': 12, '!': 13, 'appetizers': 14}\n",
      "{'all': 0, 'the': 1, 'and': 2, 'salads': 3, 'were': 4, 'fabulous': 5, ',': 6, 'steak': 7, 'was': 8, 'mouth': 9, 'watering': 10, 'pasta': 11, 'delicious': 12, '!': 13, 'appetizers': 14}\n",
      "{'all': 0, 'the': 1, 'and': 2, 'salads': 3, 'were': 4, 'fabulous': 5, ',': 6, 'steak': 7, 'was': 8, 'mouth': 9, 'watering': 10, 'pasta': 11, 'delicious': 12, '!': 13, 'appetizers': 14}\n",
      "{'all': 0, 'the': 1, 'and': 2, 'salads': 3, 'were': 4, 'fabulous': 5, ',': 6, 'steak': 7, 'was': 8, 'mouth': 9, 'watering': 10, 'pasta': 11, 'delicious': 12, '!': 13, 'appetizers': 14}\n",
      "{'all': 0, 'the': 1, 'and': 2, 'salads': 3, 'were': 4, 'fabulous': 5, ',': 6, 'steak': 7, 'was': 8, 'mouth': 9, 'watering': 10, 'pasta': 11, 'delicious': 12, '!': 13, 'appetizers': 14}\n",
      "{'all': 0, 'the': 1, 'and': 2, 'salads': 3, 'were': 4, 'fabulous': 5, ',': 6, 'steak': 7, 'was': 8, 'mouth': 9, 'watering': 10, 'pasta': 11, 'delicious': 12, '!': 13, 'appetizers': 14}\n",
      "{'all': 0, 'the': 1, 'and': 2, 'salads': 3, 'were': 4, 'fabulous': 5, ',': 6, 'steak': 7, 'was': 8, 'mouth': 9, 'watering': 10, 'pasta': 11, 'delicious': 12, '!': 13, 'appetizers': 14}\n",
      "{'all': 0, 'the': 1, 'and': 2, 'salads': 3, 'were': 4, 'fabulous': 5, ',': 6, 'steak': 7, 'was': 8, 'mouth': 9, 'watering': 10, 'pasta': 11, 'delicious': 12, '!': 13, 'appetizers': 14}\n",
      "{'all': 0, 'the': 1, 'and': 2, 'salads': 3, 'were': 4, 'fabulous': 5, ',': 6, 'steak': 7, 'was': 8, 'mouth': 9, 'watering': 10, 'pasta': 11, 'delicious': 12, '!': 13, 'appetizers': 14}\n",
      "{'all': 0, 'the': 1, 'and': 2, 'salads': 3, 'were': 4, 'fabulous': 5, ',': 6, 'steak': 7, 'was': 8, 'mouth': 9, 'watering': 10, 'pasta': 11, 'delicious': 12, '!': 13, 'appetizers': 14}\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    for i in range(20):\n",
    "        emb._add_word_('All the appetizers and salads were fabulous, the steak was mouth watering and the pasta was delicious!!!')\n",
    "        print(emb._get_dic_())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E_all = emb.embedding['all']\n",
    "type(E_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40.67011703049458"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(E_all, E_all).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54.210771477027805"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E_the = emb.embedding['the']\n",
    "np.dot(E_the, E_the).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00986528236536729"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E_app = emb.embedding['appetizers']\n",
    "np.dot(E_app, E_app).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.09201740e-03, -5.13712532e-03, -3.19205056e-03, -1.12457257e-03,\n",
       "       -4.53393842e-03,  6.99012226e-03,  6.47041221e-03,  7.09443739e-04,\n",
       "       -8.34251975e-03,  1.49655140e-03, -3.32683062e-03,  9.85519760e-03,\n",
       "       -3.03301902e-03, -2.10983893e-03,  1.09938106e-03,  5.06186232e-03,\n",
       "       -7.36367169e-03,  9.42059871e-03, -4.40424982e-03,  2.04025071e-03,\n",
       "        4.92205153e-03,  3.36179628e-03, -2.98355201e-03,  1.56471938e-03,\n",
       "       -6.06411934e-03, -1.04457344e-03,  4.35706646e-03,  7.39735786e-03,\n",
       "        9.93128600e-03, -3.64700902e-03,  4.84594567e-03, -3.78971697e-03,\n",
       "       -5.02008490e-03, -2.67187010e-03, -1.43062002e-03,  1.94243282e-03,\n",
       "       -8.24167631e-03,  3.00631260e-03,  2.71851105e-03,  9.75263780e-03,\n",
       "       -1.49531785e-05,  7.64968197e-03, -6.52892709e-04,  8.12582885e-03,\n",
       "        6.30999275e-04, -3.19742068e-03,  3.49292842e-03, -9.06613473e-03,\n",
       "        9.05621505e-03, -4.44274658e-03, -1.47550979e-03,  8.57886749e-03,\n",
       "       -5.61718981e-03,  7.16572997e-03, -1.37413788e-03, -1.84043998e-03,\n",
       "       -4.85443131e-03,  9.91196059e-05,  4.55486496e-03,  6.78028488e-03,\n",
       "       -9.88050404e-03,  3.16163523e-03, -2.70666571e-03,  2.22571619e-03,\n",
       "       -6.36602609e-03, -8.14853557e-03, -2.09036612e-03, -2.50508464e-03,\n",
       "       -3.72066679e-03, -6.36297589e-04,  8.51389762e-03,  3.14089732e-03,\n",
       "        2.69592572e-03, -5.29887577e-03, -1.82010841e-03, -3.60905007e-03,\n",
       "       -5.63974464e-03,  2.29553280e-03, -5.50692034e-03, -6.90218034e-03,\n",
       "        6.03316207e-03,  3.09600828e-03, -7.89785788e-03, -7.24533613e-03,\n",
       "        1.77991412e-03, -1.76403500e-03, -9.13268854e-03,  6.73261406e-03,\n",
       "       -3.28109725e-03, -7.01989531e-03,  4.93826986e-03, -8.02772067e-03,\n",
       "       -7.29467256e-04,  8.82026361e-03, -5.46669490e-03,  7.12946669e-03,\n",
       "        7.49642164e-03, -2.76886228e-03,  2.37955221e-03, -8.48670105e-03,\n",
       "       -4.98876910e-03,  6.87709973e-03, -9.18797597e-03,  3.39025744e-03,\n",
       "        6.29774126e-03, -1.51817329e-03,  9.43215986e-03,  9.76376195e-03,\n",
       "       -6.53405206e-03, -1.89691683e-03,  7.11090829e-03,  6.07028172e-03,\n",
       "       -4.32787911e-03,  9.39271335e-03,  1.47348254e-03, -6.05062140e-03,\n",
       "        2.29736869e-03, -2.61397375e-03,  6.56115954e-03,  5.78170193e-03,\n",
       "       -3.46658468e-03, -1.32010988e-03,  9.29844220e-03, -9.22337532e-03,\n",
       "        7.04721367e-03, -7.78869265e-03, -2.36749481e-03,  6.31456647e-04,\n",
       "       -2.14079027e-03, -8.25895193e-03, -3.04592149e-03, -3.24178441e-03,\n",
       "        4.07775376e-03, -2.54732823e-04,  5.39821288e-04, -4.45644039e-03,\n",
       "        2.11741309e-03,  5.88748149e-03,  7.48587629e-03,  9.53891483e-03,\n",
       "        7.32725535e-04,  9.18484712e-03, -9.57467651e-03,  5.99630208e-03,\n",
       "        9.89315974e-03,  4.00793666e-03, -5.74665638e-03, -7.11924030e-03,\n",
       "        4.97828901e-03,  2.04346422e-03, -8.06090893e-03,  4.36087361e-03,\n",
       "        3.92536605e-03, -8.75038681e-04,  1.80747729e-03,  9.11542227e-03,\n",
       "        9.60870754e-03,  1.63965183e-04,  9.20011159e-03,  6.07422451e-03,\n",
       "       -8.73624487e-03, -6.93754169e-03,  4.22153165e-03,  2.41827886e-03,\n",
       "       -8.87332743e-03, -3.39719505e-03,  2.47869082e-04, -8.32649074e-03,\n",
       "       -5.26413958e-03,  3.90345290e-03,  1.19088694e-03, -6.55599122e-03,\n",
       "       -3.86758178e-03,  1.56575030e-03, -4.30172782e-05,  5.00847088e-03,\n",
       "        9.24049620e-03,  3.16713626e-03,  2.01187134e-03,  6.91649793e-03,\n",
       "        4.70009621e-03,  2.46360864e-03, -9.03762331e-03,  2.16230522e-03,\n",
       "       -5.17879498e-03,  5.36727902e-03, -2.63929181e-03, -2.76300149e-03,\n",
       "        2.06953046e-04,  1.26039907e-03, -4.00357478e-03, -5.38183262e-03,\n",
       "       -9.97234089e-03,  9.55158682e-03,  6.49278128e-04, -9.40521990e-03,\n",
       "       -2.42314505e-03, -2.89873418e-03,  1.39954045e-03, -2.63726393e-03,\n",
       "        2.88791989e-03, -3.17510500e-03, -2.12752111e-03,  8.01829879e-03,\n",
       "        1.70465718e-03,  6.36891568e-03,  5.18477002e-03,  8.91991924e-03,\n",
       "        1.02779256e-03,  6.35717090e-03, -1.93224356e-03,  7.54083675e-03,\n",
       "        4.93861082e-03, -8.59077099e-03,  9.69373613e-04,  1.60283457e-03,\n",
       "        3.49229487e-03, -8.28268509e-03,  1.56013615e-04, -2.49417836e-03,\n",
       "       -4.90586757e-03,  7.29248890e-03,  3.37667634e-03,  2.59404564e-03,\n",
       "       -2.52479214e-03, -1.48964993e-03, -1.21794660e-03, -5.28243204e-03,\n",
       "       -3.83419329e-03,  2.83166169e-03,  2.46256972e-03, -1.29601287e-03,\n",
       "        8.01486892e-03,  3.23144288e-03, -5.92718791e-03, -8.75214718e-03,\n",
       "        1.03857807e-03,  8.36297006e-03, -9.91804399e-03, -2.64023316e-03,\n",
       "       -1.15001968e-03,  5.24026941e-03, -5.00921880e-03, -9.73408008e-03,\n",
       "        9.78081814e-03,  8.93790335e-03,  5.63079235e-03,  5.84955274e-03,\n",
       "        6.83028461e-03,  5.77862502e-03, -5.12641677e-03,  9.62991573e-03,\n",
       "        6.78945180e-03,  1.46794162e-03, -8.73904538e-03,  9.46575644e-03,\n",
       "       -2.05455885e-04, -1.07209878e-03,  2.14694119e-03, -1.86789281e-03,\n",
       "       -5.47649694e-03, -7.97519217e-03, -6.58169955e-03, -7.82672508e-03,\n",
       "        5.73750411e-03,  4.72698219e-03,  8.60227720e-03, -9.01771920e-03,\n",
       "       -7.91416841e-03, -8.13698854e-03, -2.21033919e-04,  8.70487174e-03,\n",
       "        8.31539118e-03, -4.77561913e-03, -5.60767659e-03,  2.51683655e-04,\n",
       "       -5.85005006e-03,  1.39116214e-03,  8.23849224e-03, -7.42783841e-03,\n",
       "       -4.07716059e-03,  1.77186543e-03,  2.11148065e-04,  4.10523138e-03,\n",
       "        9.59485942e-03,  6.23017022e-03,  8.84612514e-03,  7.09903200e-03,\n",
       "        9.51014224e-03,  9.38735448e-03, -4.87832002e-03, -6.39841097e-03,\n",
       "       -2.15046973e-03, -3.50189444e-03,  9.37859827e-03,  4.96435550e-03,\n",
       "       -6.83506131e-03,  8.06434740e-03, -1.60733543e-03,  9.19735984e-03])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E_app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.15063  , -0.73908  , -0.29427  , -0.30443  ,  0.16372  ,\n",
       "       -0.51402  , -0.12802  ,  0.3488   ,  0.16604  ,  0.30374  ,\n",
       "       -0.44225  ,  0.63938  , -0.03751  ,  0.18215  ,  0.21155  ,\n",
       "        0.13111  , -0.40701  ,  1.7404   , -0.19956  ,  0.13135  ,\n",
       "       -0.061498 , -0.39734  ,  0.1863   , -0.18395  ,  0.21751  ,\n",
       "       -0.17888  , -0.23852  , -0.0082472, -0.17775  , -0.074496 ,\n",
       "       -0.1261   ,  0.78656  , -0.065985 ,  0.054162 ,  0.13414  ,\n",
       "        0.20414  , -0.33127  , -0.37488  ,  0.047762 ,  1.0582   ,\n",
       "        0.034112 ,  0.43122  ,  0.48421  ,  0.40248  ,  0.019783 ,\n",
       "        0.071812 ,  0.41956  ,  0.12729  ,  0.30718  , -0.34761  ,\n",
       "       -0.20851  , -0.62237  , -0.50255  , -0.14081  ,  0.026269 ,\n",
       "        0.071628 , -0.083321 , -0.24162  , -0.3319   , -0.20401  ,\n",
       "        0.34444  ,  0.060308 , -0.2042   , -0.086079 ,  0.057419 ,\n",
       "        0.12683  ,  0.151    ,  0.10077  ,  0.35313  ,  0.25843  ,\n",
       "        0.35271  ,  0.28722  ,  0.79539  , -0.15875  ,  0.24777  ,\n",
       "        0.25978  , -0.013694 , -0.39059  ,  0.3512   , -0.075469 ,\n",
       "        0.075343 , -0.37291  , -0.061633 , -0.43695  ,  0.39346  ,\n",
       "       -0.11588  , -0.57071  , -1.5943   ,  0.65575  ,  0.065577 ,\n",
       "       -0.063779 ,  0.28942  , -0.081619 , -0.11387  ,  0.061768 ,\n",
       "       -0.44001  , -0.26626  ,  0.094261 , -0.066784 , -0.03106  ,\n",
       "        0.24153  , -0.067925 , -0.013224 , -0.57601  ,  0.11462  ,\n",
       "        0.51861  ,  0.10161  , -0.37564  , -0.19759  ,  0.072547 ,\n",
       "       -0.073661 ,  0.29698  ,  0.18207  , -0.39983  , -0.26075  ,\n",
       "       -0.28167  ,  0.040006 ,  0.27181  , -0.19596  , -0.53371  ,\n",
       "        0.057205 ,  0.058041 ,  0.18503  , -0.053689 ,  1.1685   ,\n",
       "       -0.31149  ,  0.12289  ,  0.61495  ,  0.055731 ,  0.19277  ,\n",
       "       -0.11354  , -0.16036  , -0.63883  , -0.35782  , -0.35044  ,\n",
       "        0.47043  , -0.15249  ,  0.23638  ,  0.35732  ,  0.6472   ,\n",
       "       -0.92337  ,  0.10009  ,  0.34918  ,  0.70989  , -0.029512 ,\n",
       "        0.20197  , -0.27458  , -0.11499  , -0.16327  , -0.15457  ,\n",
       "       -0.4005   ,  0.32747  , -0.22412  ,  0.01356  ,  0.28882  ,\n",
       "        0.30868  , -0.29599  ,  0.13477  , -0.1836   ,  0.12078  ,\n",
       "       -0.37952  ,  0.10588  , -0.13348  , -0.3427   , -0.14737  ,\n",
       "        0.10091  , -0.24907  , -0.39966  , -0.46489  , -0.31774  ,\n",
       "       -0.064117 ,  0.2571   , -0.065229 , -0.34949  ,  0.11163  ,\n",
       "        0.57793  , -0.27001  ,  0.40832  ,  0.39851  ,  0.088102 ,\n",
       "        0.014602 , -0.031864 , -0.21875  ,  0.59167  , -0.072989 ,\n",
       "       -0.42064  , -0.014209 , -0.35342  ,  0.44309  ,  0.30545  ,\n",
       "        0.34371  , -0.60571  , -0.087255 ,  0.16219  , -0.19577  ,\n",
       "       -0.15955  , -0.52208  ,  0.2328   , -0.23237  ,  0.24357  ,\n",
       "       -0.18899  ,  0.26539  ,  0.13896  , -0.13802  , -0.055697 ,\n",
       "       -0.13135  ,  0.35695  , -0.0085761,  0.74357  , -0.26168  ,\n",
       "        0.33939  ,  0.17978  , -0.096515 ,  0.80578  ,  0.60643  ,\n",
       "        0.15557  ,  0.12843  , -1.8185   , -0.94236  ,  0.35904  ,\n",
       "        0.19001  ,  0.23029  , -0.19397  , -0.4399   ,  0.089338 ,\n",
       "       -0.20172  , -0.67754  ,  0.65915  , -0.35645  , -0.31299  ,\n",
       "       -0.056985 , -0.21336  ,  0.018703 , -0.067952 ,  0.22129  ,\n",
       "        0.42823  ,  0.37159  ,  0.077299 , -0.19032  , -0.18498  ,\n",
       "       -0.42464  , -0.46186  , -0.10955  ,  0.20688  ,  0.1465   ,\n",
       "        0.38881  , -0.18061  ,  0.045229 , -0.01485  ,  0.94683  ,\n",
       "        0.19614  ,  0.17191  , -0.17449  , -0.31834  , -0.31428  ,\n",
       "        0.70659  , -0.13587  , -0.029818 , -0.23358  ,  0.32676  ,\n",
       "       -0.31906  , -0.10308  ,  0.13386  ,  0.027616 , -0.24585  ,\n",
       "        0.10326  , -0.16011  ,  0.28729  ,  0.73849  , -0.10718  ,\n",
       "       -0.44154  , -0.16639  , -0.13104  , -0.32071  ,  0.2608   ,\n",
       "       -0.10815  , -0.0096083,  0.30584  , -0.29823  ,  0.17412  ,\n",
       "        0.23775  ,  0.037106 , -0.19637  ,  0.69878  ,  0.0032808,\n",
       "        0.31604  , -0.088712 , -0.18784  , -0.18948  , -0.012417 ,\n",
       "       -0.4138   , -0.14307  , -0.2437   , -0.35445  , -0.12416  ,\n",
       "        0.073719 , -0.25675  , -0.28298  ,  0.085819 , -0.04622  ])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
