{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coding:utf-8\n",
    "import torch as t\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import ipdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding : have input words : 1\n",
      "Embedding : have input words : 2\n",
      "Embedding : have input words : 4\n",
      "Embedding : have input words : 8\n",
      "Embedding : have input words : 16\n",
      "Embedding : have input words : 32\n",
      "Embedding : have input words : 64\n",
      "Embedding : have input words : 128\n",
      "Embedding : have input words : 256\n",
      "Embedding : have input words : 512\n",
      "Embedding : have input words : 1024\n",
      "Embedding : have input words : 2048\n",
      "Embedding : have input words : 4096\n",
      "Embedding : have input words : 8192\n",
      "Embedding : have input words : 16384\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('D:\\\\Jupyter\\\\Python\\\\ATAE-LSTM')\n",
    "import Ipynb_importer\n",
    "from config import opt\n",
    "from data.Embedding import emb\n",
    "from models.BasicModule import BasicModule\n",
    "from data.AspClas_ import AspClas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AE_GRU(BasicModule):\n",
    "    def __init__(self):\n",
    "        super(AE_GRU, self).__init__()\n",
    "        \n",
    "        self.embedding = emb._make_layer_()\n",
    "        \n",
    "        self.gru = nn.GRU(opt.hidden_size*2, opt.hidden_size, 1)\n",
    "        self.h0 = t.randn(1, 1, opt.hidden_size)\n",
    "        \n",
    "        self.lin = nn.Linear(opt.hidden_size, opt.classes)\n",
    "        self.tanh = t.nn.Tanh()\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # word representation\n",
    "        w = x[0]\n",
    "        N = len(w)\n",
    "        # aspect term\n",
    "        v = x[1]\n",
    "        # assert len(v)==1  # use mean()\n",
    "        \n",
    "        # e.g.\n",
    "        # w torch.Size([16])\n",
    "        # v torch.Size([1])\n",
    "        \n",
    "        e1 = self.embedding(x[0])\n",
    "        e2 = self.embedding(x[1]).mean(dim=0).view(-1).expand(e1.size())\n",
    "        # e.g.\n",
    "        # e1 torch.Size([16, 300])\n",
    "        # e2 torch.Size([1, 300]) -> torch.Size([16, 300])\n",
    "        \n",
    "        wv = t.cat((e1.view(N,1,opt.hidden_size), e2.view(N,1,opt.hidden_size)), dim=-1)\n",
    "        # e.g.\n",
    "        # wv torch.Size([16, 1, 600])\n",
    "        \n",
    "        out, hn = self.gru(wv, self.h0)\n",
    "        # e.g.\n",
    "        # out torch.Size([16, 1, 300])\n",
    "        # hn torch.Size([1, 1, 300])\n",
    "        \n",
    "        y = self.softmax(self.lin(hn.view(opt.hidden_size)))\n",
    "        # e.g.\n",
    "        # y torch.Size([3])\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned ON\n",
      "tensor([0.2366, 0.4165, 0.3469], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.3710, 0.3484, 0.2806], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.3815, 0.3369, 0.2816], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.2799, 0.3123, 0.4078], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.3329, 0.3325, 0.3347], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.2787, 0.3304, 0.3909], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.3025, 0.3696, 0.3279], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.2767, 0.3343, 0.3890], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.3337, 0.3564, 0.3099], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.2754, 0.3672, 0.3574], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.3447, 0.3318, 0.3236], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.3055, 0.4136, 0.2809], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.3304, 0.3402, 0.3294], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.3205, 0.3747, 0.3048], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.3359, 0.3548, 0.3093], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.2808, 0.2990, 0.4202], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.3173, 0.3507, 0.3320], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.2992, 0.3612, 0.3396], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.2685, 0.3031, 0.4284], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.2774, 0.3235, 0.3992], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.3259, 0.3591, 0.3149], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.3552, 0.3279, 0.3169], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.2928, 0.3718, 0.3354], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.2912, 0.3286, 0.3802], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.3005, 0.3675, 0.3320], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.3259, 0.3285, 0.3457], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.3056, 0.3676, 0.3268], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.2544, 0.3745, 0.3712], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.2704, 0.3045, 0.4251], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.3654, 0.3132, 0.3215], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.3709, 0.3381, 0.2910], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.2773, 0.4019, 0.3209], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.2662, 0.2980, 0.4357], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.3041, 0.3625, 0.3334], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.3241, 0.4012, 0.2747], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.3018, 0.3811, 0.3171], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.3026, 0.3611, 0.3364], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.2732, 0.3204, 0.4063], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.3337, 0.3439, 0.3224], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.4017, 0.2821, 0.3162], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.3675, 0.3792, 0.2532], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.3001, 0.3363, 0.3636], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.3081, 0.3701, 0.3218], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.2817, 0.3203, 0.3980], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.2658, 0.3553, 0.3789], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.3355, 0.3463, 0.3182], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.2741, 0.3182, 0.4076], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.2566, 0.3533, 0.3901], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.3000, 0.3202, 0.3798], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.3036, 0.3525, 0.3440], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.2846, 0.3203, 0.3950], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.2693, 0.2935, 0.4372], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.2770, 0.3262, 0.3968], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.3417, 0.3266, 0.3318], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.2669, 0.3124, 0.4207], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.3171, 0.3623, 0.3206], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.2991, 0.3389, 0.3619], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.3149, 0.3579, 0.3272], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.3656, 0.3574, 0.2770], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.3257, 0.3398, 0.3345], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.3395, 0.3385, 0.3220], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.3622, 0.3064, 0.3314], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.3191, 0.3682, 0.3127], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.3537, 0.3358, 0.3106], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.3543, 0.3818, 0.2639], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.3034, 0.3676, 0.3290], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.3050, 0.3663, 0.3287], grad_fn=<SoftmaxBackward>)\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    %pdb on\n",
    "    testDataset = AspClas(opt.base_root+'/data/restaurants-trial.xml')\n",
    "    model = AE_GRU()\n",
    "    for text, aspect, sentiment in testDataset:\n",
    "        x = (text, aspect)\n",
    "        y = model(x)\n",
    "        print(y)\n",
    "    print(len(list(model.parameters())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
