{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coding:utf-8\n",
    "import torch as t\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import ipdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding : have input words : 1\n",
      "Embedding : have input words : 2\n",
      "Embedding : have input words : 4\n",
      "Embedding : have input words : 8\n",
      "Embedding : have input words : 16\n",
      "Embedding : have input words : 32\n",
      "Embedding : have input words : 64\n",
      "Embedding : have input words : 128\n",
      "Embedding : have input words : 256\n",
      "Embedding : have input words : 512\n",
      "Embedding : have input words : 1024\n",
      "Embedding : have input words : 2048\n",
      "Embedding : have input words : 4096\n",
      "Embedding : have input words : 8192\n",
      "Embedding : have input words : 16384\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('D:\\\\Jupyter\\\\Python\\\\ATAE-LSTM')\n",
    "import Ipynb_importer\n",
    "from config import opt\n",
    "from data.Embedding import emb\n",
    "from models.BasicModule import BasicModule\n",
    "from data.AspClas_ import AspClas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from D:\\Jupyter\\Python\\MyGRU\\MyGRU.ipynb\n",
      "Automatic pdb calling has been turned ON\n"
     ]
    }
   ],
   "source": [
    "sys.path.append('D:\\\\Jupyter\\\\Python')\n",
    "from MyGRU.MyGRU import MyGRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AE_MyGRU(BasicModule):\n",
    "    def __init__(self):\n",
    "        super(AE_MyGRU, self).__init__()\n",
    "        \n",
    "        self.embedding = emb._make_layer_()\n",
    "        \n",
    "        self.gru = MyGRU(opt.hidden_size*2, opt.hidden_size)\n",
    "        self.h0 = t.randn(1, opt.hidden_size)\n",
    "        \n",
    "        self.lin = nn.Linear(opt.hidden_size, opt.classes)\n",
    "        self.tanh = t.nn.Tanh()\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # word representation\n",
    "        w = x[0]\n",
    "        N = len(w)\n",
    "        # aspect term\n",
    "        v = x[1]\n",
    "        # assert len(v)==1  # use mean()\n",
    "        \n",
    "        # e.g.\n",
    "        # w torch.Size([16])\n",
    "        # v torch.Size([1])\n",
    "        \n",
    "        e1 = self.embedding(x[0])\n",
    "        e2 = self.embedding(x[1]).mean(dim=0).view(-1).expand(e1.size())\n",
    "        # e.g.\n",
    "        # e1 torch.Size([16, 300])\n",
    "        # e2 torch.Size([1, 300]) -> torch.Size([16, 300])\n",
    "        \n",
    "        wv = t.cat((e1.view(N,1,opt.hidden_size), e2.view(N,1,opt.hidden_size)), dim=-1)\n",
    "        # e.g.\n",
    "        # wv torch.Size([16, 1, 600])\n",
    "        \n",
    "        out, hn = self.gru([wv, self.h0])\n",
    "        # e.g.\n",
    "        # out torch.Size([16, 1, 300])\n",
    "        # hn torch.Size([1, 1, 300])\n",
    "        \n",
    "        y = self.softmax(self.lin(hn.view(opt.hidden_size)))\n",
    "        # e.g.\n",
    "        # y torch.Size([3])\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned ON\n",
      "tensor([0.2178, 0.2391, 0.5430], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.2510, 0.5917, 0.1573], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.3761, 0.2859, 0.3380], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.3287, 0.1883, 0.4830], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.2324, 0.4354, 0.3323], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.1823, 0.4187, 0.3990], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.2554, 0.3386, 0.4059], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.2344, 0.3226, 0.4431], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.1437, 0.4123, 0.4440], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.2345, 0.2687, 0.4968], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.2151, 0.1001, 0.6849], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.2350, 0.5388, 0.2261], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.5436, 0.2525, 0.2040], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.2174, 0.4648, 0.3178], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.2675, 0.2823, 0.4502], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.1817, 0.2311, 0.5872], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.3179, 0.3799, 0.3022], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.4266, 0.5195, 0.0539], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.3143, 0.3080, 0.3777], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.2009, 0.1926, 0.6065], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.2360, 0.4568, 0.3071], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.3398, 0.2731, 0.3872], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.1282, 0.6978, 0.1740], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.2786, 0.4033, 0.3181], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.3821, 0.5052, 0.1127], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.3577, 0.3120, 0.3302], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.1949, 0.4024, 0.4028], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.2373, 0.5199, 0.2429], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.4066, 0.1429, 0.4505], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.1651, 0.6418, 0.1931], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.3083, 0.4287, 0.2630], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.3637, 0.4396, 0.1966], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.2432, 0.2033, 0.5535], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.3815, 0.1524, 0.4661], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.5080, 0.2966, 0.1955], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.2120, 0.6148, 0.1733], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.2084, 0.4789, 0.3127], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.3319, 0.2858, 0.3823], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.3243, 0.3279, 0.3477], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.3722, 0.4050, 0.2228], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.2998, 0.3971, 0.3031], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.2187, 0.5448, 0.2364], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.2851, 0.4798, 0.2351], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.3361, 0.2960, 0.3680], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.2610, 0.1328, 0.6062], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.4130, 0.2397, 0.3473], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.2404, 0.2447, 0.5149], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.2865, 0.4197, 0.2938], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.4474, 0.2105, 0.3421], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.2193, 0.4169, 0.3638], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.4670, 0.1310, 0.4020], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.3287, 0.2937, 0.3776], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.2474, 0.4613, 0.2912], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.3155, 0.2028, 0.4816], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.4371, 0.2658, 0.2970], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.1474, 0.5650, 0.2875], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.2383, 0.4038, 0.3579], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.1965, 0.6133, 0.1902], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.5616, 0.1677, 0.2707], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.2594, 0.3407, 0.3999], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.3362, 0.4129, 0.2510], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.2557, 0.4189, 0.3255], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.1593, 0.3294, 0.5113], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.2567, 0.3108, 0.4324], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.2965, 0.3267, 0.3768], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.4296, 0.2134, 0.3570], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.3494, 0.3946, 0.2561], grad_fn=<SoftmaxBackward>)\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    %pdb on\n",
    "    testDataset = AspClas(opt.base_root+'/data/restaurants-trial.xml')\n",
    "    model = AE_MyGRU()\n",
    "    for text, aspect, sentiment in testDataset:\n",
    "        x = (text, aspect)\n",
    "        y = model(x)\n",
    "        print(y)\n",
    "    print(len(list(model.parameters())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
